{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1zSFns-1ZQK",
    "outputId": "38c00b1d-3b28-4b1e-dd8d-862d289f6e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID                                        resume_text Category\n",
      "0  16852973  HR ADMINISTRATOR/MARKETING ASSOCIATE\\n\\nHR ADM...       HR\n",
      "1  22323967  HR SPECIALIST, US HR OPERATIONS       Summary ...       HR\n",
      "2  33176873  HR DIRECTOR       Summary      Over 20 years e...       HR\n",
      "3  27018550  HR SPECIALIST       Summary    Dedicated, Driv...       HR\n",
      "4  17812897  HR MANAGER         Skill Highlights           ...       HR\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/pro/Resume.csv')\n",
    "\n",
    "# Function to extract plain text from HTML\n",
    "def html_to_text(html):\n",
    "    if pd.isna(html):\n",
    "        return \"\"\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Apply extraction to create a clean resume text column\n",
    "df['resume_text'] = df.apply(\n",
    "    lambda row: row['Resume_str'].strip() if pd.notna(row['Resume_str']) and row['Resume_str'].strip() != \"\"\n",
    "    else html_to_text(row['Resume_html']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "extracted_df = df[['ID', 'resume_text', 'Category']].copy()\n",
    "\n",
    "# Display sample\n",
    "print(extracted_df.head())\n",
    "\n",
    "# Save to new CSV\n",
    "extracted_df.to_csv('extracted_resumes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm7IK7kZ1_PC",
    "outputId": "fe1a8bfb-1ffb-4594-945c-3c4bf1de5f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID                                  resume_text_clean Category  label\n",
      "0  16852973  HR ADMINISTRATOR/MARKETING ASSOCIATE HR ADMINI...       HR    0.4\n",
      "1  22323967  HR SPECIALIST, US HR OPERATIONS Summary Versat...       HR    0.4\n",
      "2  33176873  HR DIRECTOR Summary Over 20 Years Experience I...       HR    0.4\n",
      "3  27018550  HR SPECIALIST Summary Dedicated, Driven, And D...       HR    0.4\n",
      "4  17812897  HR MANAGER Skill Highlights HR SKILLS HR Depar...       HR    0.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the data (exported from Numbers or downloaded CSV)\n",
    "df = pd.read_csv('extracted_resumes.csv')\n",
    "\n",
    "# 2. Clean the resume_text column\n",
    "def clean_text(text):\n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\r\\t\\x0b\\x0c]', ' ', text)\n",
    "    # Collapse multiple newlines and spaces\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'[ ]{2,}', ' ', text)\n",
    "    # Normalize punctuation spacing (e.g., \"word , word\" \u2192 \"word, word\")\n",
    "    text = re.sub(r'\\s+([,.:;!?])', r'\\1', text)\n",
    "    text = re.sub(r'([,.:;!?])([^\\s])', r'\\1 \\2', text)\n",
    "    # Trim leading/trailing whitespace on each line\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    # Rejoin lines with a single newline\n",
    "    cleaned = '\\n'.join([line for line in lines if line])\n",
    "    # Smart casing: preserve acronyms, lowercase the rest\n",
    "    def smart_case(s):\n",
    "        return ' '.join([w if w.isupper() else w.capitalize() for w in s.split()])\n",
    "    return smart_case(cleaned)\n",
    "\n",
    "df['resume_text_clean'] = df['resume_text'].astype(str).apply(clean_text)\n",
    "\n",
    "# 3. Verify/annotate relevance labels\n",
    "# Assuming source data from the Kaggle notebook provides a 'label' column\n",
    "# Confirm scale consistency (should be between 0.0 and 1.0)\n",
    "if 'label' in df.columns:\n",
    "    # Clip to [0,1] and convert to float\n",
    "    df['label'] = pd.to_numeric(df['label'], errors='coerce').clip(0.0, 1.0)\n",
    "else:\n",
    "    # Derive weak labels by simple keyword matching on job titles\n",
    "    # Example: rows where 'Manager' in Category get higher base score\n",
    "    def weak_label(row):\n",
    "        category = str(row['Category']).lower()\n",
    "        if 'director' in category or 'manager' in category:\n",
    "            return 0.8\n",
    "        if 'specialist' in category or 'administrator' in category:\n",
    "            return 0.6\n",
    "        return 0.4\n",
    "    df['label'] = df.apply(weak_label, axis=1)\n",
    "\n",
    "# 4. Preview results\n",
    "print(df[['ID', 'resume_text_clean', 'Category', 'label']].head())\n",
    "\n",
    "# 5. Save cleaned data\n",
    "df.to_csv('cleaned_resumes_with_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYjzsQAC2Bcb",
    "outputId": "2394ce7d-1e44-43d1-93be-aab773a43526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID Category                                    job_description\n",
      "0  16852973       HR  Responsible for recruitment, employee relation...\n",
      "1  22323967       HR  Responsible for recruitment, employee relation...\n",
      "2  33176873       HR  Responsible for recruitment, employee relation...\n",
      "3  27018550       HR  Responsible for recruitment, employee relation...\n",
      "4  17812897       HR  Responsible for recruitment, employee relation...\n",
      "5  11592605       HR  Responsible for recruitment, employee relation...\n",
      "6  25824789       HR  Responsible for recruitment, employee relation...\n",
      "7  15375009       HR  Responsible for recruitment, employee relation...\n",
      "8  11847784       HR  Responsible for recruitment, employee relation...\n",
      "9  32896934       HR  Responsible for recruitment, employee relation...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load data\n",
    "cleaned_df = pd.read_csv('cleaned_resumes_with_labels.csv')\n",
    "job_df     = pd.read_csv('/content/drive/MyDrive/pro/training_data 2.csv')\n",
    "\n",
    "# 2. Normalize merge keys\n",
    "cleaned_df['Category_norm'] = cleaned_df['Category'].str.lower().str.strip()\n",
    "job_df['company_norm']      = job_df['company_name'].str.lower().str.strip()\n",
    "\n",
    "# 3. Merge on normalized keys\n",
    "merged = cleaned_df.merge(\n",
    "    job_df[['company_norm', 'job_description']],\n",
    "    left_on='Category_norm',\n",
    "    right_on='company_norm',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Define generic templates per category\n",
    "default_templates = {\n",
    "    'hr': \"Responsible for recruitment, employee relations, and compliance.\",\n",
    "    'software engineer': \"Develop and maintain software applications using modern languages.\",\n",
    "    'data scientist': \"Analyze data, build predictive models, and derive business insights.\",\n",
    "    'product manager': \"Define product roadmap, gather requirements, and coordinate cross-functional teams.\",\n",
    "    'marketing specialist': \"Plan and execute digital marketing campaigns and analyze performance metrics.\",\n",
    "    'sales representative': \"Identify leads, engage prospects, and close sales deals.\",\n",
    "    'business analyst': \"Gather business requirements and perform data-driven analysis.\",\n",
    "    'devops engineer': \"Implement CI/CD pipelines and manage cloud infrastructure.\",\n",
    "    'ux designer': \"Conduct user research and create wireframes and prototypes.\",\n",
    "    'financial analyst': \"Perform financial modeling and reporting to support decision-making.\"\n",
    "}\n",
    "\n",
    "# 5. Fill job_description with generic if missing\n",
    "merged['job_description'] = merged.apply(\n",
    "    lambda row: row['job_description']\n",
    "                if pd.notna(row['job_description'])\n",
    "                else default_templates.get(row['Category_norm'], \"\"),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 6. Drop helper columns\n",
    "final_df = merged.drop(columns=['Category_norm', 'company_norm'])\n",
    "\n",
    "# 7. Save the result\n",
    "final_df.to_csv('resumes_with_fallback_job_descriptions.csv', index=False)\n",
    "\n",
    "# 8. Preview\n",
    "print(final_df[['ID', 'Category', 'job_description']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "919db896"
   },
   "outputs": [],
   "source": [
    "# 1. Install dependencies\n",
    "!pip install -q transformers peft PyPDF2\n",
    "\n",
    "# 2. Imports\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from google.colab import files\n",
    "\n",
    "# 3. Load your fine-tuned model and tokenizer\n",
    "model_name_or_path = \"./phi3mini_finetuned\"  # or your fine-tuned directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# 4. PDF text extraction utility\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> str:\n",
    "    from PyPDF2 import PdfReader\n",
    "    reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# 5. Prediction helper\n",
    "def predict(model, tokenizer, prompt: str, max_length: int = 512) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. Upload resume PDF\n",
    "print(\"\ud83d\udcc4 Upload your resume PDF file:\")\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded.\")\n",
    "resume_bytes = next(iter(uploaded.values()))\n",
    "resume_text = extract_text_from_pdf(resume_bytes)\n",
    "\n",
    "# 7. Input job description\n",
    "print(\"\\n\u2702\ufe0f Paste the job description (end with a blank line):\")\n",
    "lines = []\n",
    "while True:\n",
    "    line = input()\n",
    "    if not line.strip():\n",
    "        break\n",
    "    lines.append(line)\n",
    "job_description = \"\\n\".join(lines)\n",
    "\n",
    "# 8. Format prompt for your model\n",
    "prompt = (\n",
    "    f\"Job Description:\\n{job_description}\\n\\n\"\n",
    "    f\"Resume Text:\\n{resume_text}\\n\\n\"\n",
    "    \"You are an ATS scoring assistant. Return a JSON with key \\\"relevance_score\\\" between 0 and 100.\\n\"\n",
    "    \"Example output:\\n\"\n",
    "    \"{\\\"relevance_score\\\": 75.4}\"\n",
    ")\n",
    "\n",
    "# 9. Generate prediction\n",
    "predicted_text = predict(model, tokenizer, prompt)\n",
    "\n",
    "# 10. Extract relevance score\n",
    "predicted_score = None\n",
    "try:\n",
    "    # Try JSON parse\n",
    "    predicted_score = json.loads(predicted_text.strip()).get(\"relevance_score\")\n",
    "except json.JSONDecodeError:\n",
    "    # Fallback regex\n",
    "    match = re.search(r'\"relevance_score\"\\s*:\\s*([\\d.]+)', predicted_text)\n",
    "    if match:\n",
    "        predicted_score = float(match.group(1))\n",
    "\n",
    "# 11. Display result\n",
    "if predicted_score is not None:\n",
    "    print(f\"\\nPredicted Relevance Score: {predicted_score}\")\n",
    "else:\n",
    "    print(\"\\nCould not predict relevance score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enh6DKZ-2OcN",
    "outputId": "b8b89b95-9ca2-48f2-be73-45f16f1b356f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.3.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.10.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.56.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jsonlines, trl\n",
      "Successfully installed jsonlines-4.0.0 trl-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSyNswYH2Kwi",
    "outputId": "de80c381-96bf-4491-d017-6aec3b86f10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1987, Validation: 248, Test: 249\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "# Load merged data\n",
    "df = pd.read_csv('resumes_with_fallback_job_descriptions.csv')\n",
    "\n",
    "# Step 3: Split into train (80%), val (10%), test (10%) stratified by Category\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    stratify=df['Category'],\n",
    "    random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_df['Category'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Construct fine-tuning inputs/outputs\n",
    "def make_record(row):\n",
    "    prompt = (\n",
    "        f\"Resume: {row['resume_text_clean']}\\n\\n\"\n",
    "        f\"Job Description: {row['job_description']}\"\n",
    "    )\n",
    "    target = json.dumps({\"relevance_score\": float(row['label'])})\n",
    "    return {\"conversations\": [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": target}\n",
    "    ]}\n",
    "\n",
    "# Generate records\n",
    "splits = {'train': train_df, 'validation': val_df, 'test': test_df}\n",
    "for split_name, split_df in splits.items():\n",
    "    records = [make_record(row) for _, row in split_df.iterrows()]\n",
    "    with jsonlines.open(f\"{split_name}_data.jsonl\", mode='w') as writer:\n",
    "        writer.write_all(records)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4948acb92c9c492291b662df344d45c2",
      "d7b88c233e5545dd80fc81c4926775db",
      "39b0869c816f425c9fe45dd3e80c217b",
      "4337d402a3bf4283963fd507e0d23cf3",
      "bbc5bc7b72f042b5a4872ab71b6f670b",
      "7fea943e976d449e96684903f86e0c5e",
      "278ac5c580874d1ba231d9eb8484aa8e",
      "bc62e130adb945ac9a7409498480fd5a",
      "0416c017e3e444ed98488f55964e5879",
      "94e7aff732d74cd19e5f9b7fe33f6518",
      "6a37c24aaeb14337ab5560a81307984f",
      "6dd9b945612c42c5a5060603aea8a15f",
      "c0eb286686b84c12b6932261f8983f91",
      "43b8b224d2a24219b376318875d511ba",
      "a7e5488297b246999dcf787bc17a9219",
      "df982403c3d840d39397248bbbf704a1",
      "70d17b4625774b20af06ee47fbbc9af9",
      "c84908bff85e42ce8945f1479d4fc9f3",
      "2167598d13894531b6b0f855cc571f24",
      "ce39329c7844482886d0b267c952d0a9",
      "3962a1d13ebc4a7a8d7a6589d4dc0c34",
      "25cf80424ff6401c936c0bb168811c05",
      "b8288e24299f4078b23c4846d337deb7",
      "026d05ef46ae4d608eacc8f59d4b22b7",
      "b4d146d9ec104d249683a7fe0caa8a81",
      "54c2136b9ab84a68935d002e3c443f90",
      "a207d11576b84f3f81d9a5efc3aacd64",
      "b6ab796b1c9d4379b60ce9f433123673",
      "2fad68d336c044d782f0e1b9616a1aea",
      "8f6d21ce9f5d49cb92f98e3c50c0e613",
      "98c85e8f88ec407f9e040830a2a48951",
      "4c0598d56d464158bdc6a1c1218e0f05",
      "f4f63719e5cf4d04a3e5a739806b288e",
      "5b3260710167417a80305051f3aecd96",
      "659dd7ed3d3a470a89fbadcfa6032ee4",
      "b60afabbb6ae4409983112583de8b67a",
      "c4ad8d3cf1044ba4a0aa284c2295f467",
      "01517ab196394f3f9e2069aaeca26c43",
      "7c8d7605af7543ad817dabbf243f9a3f",
      "1b0ba22e931044de9316cea15f80372d",
      "8fd48492c51f4451aa7bcc390456644f",
      "8576c74de8144bbc8fa9c4f04b3b235f",
      "89d1da23856145a399df3f0113a3e549",
      "4a0e2262ddf54480a4560296f92efe3e",
      "8ca5d8e75d604dfebbcba8f15cc0e1aa",
      "343dc09c82ee46459f209671c2d1c32e",
      "eb5d78eb4d834bfab1ad88d69d888c1c",
      "9c9e31da6c784bf38c0ec0e412738349",
      "32ce170288db49bdaa278a9c6aedff5d",
      "d1670f8314e44f65b394e954004b5f51",
      "b6f95539222a491d94969c95bc40373a",
      "a69035093c8949cca76cb5f0fb52906b",
      "8ebf6dacda0f40d2b353747878b85153",
      "0fb19592f9434f3bbc31ac12cd1f9c6a",
      "7e79c4e0bd414285a9c89d9a7449bda1",
      "204dc53a701543d6b1403a4fc4cda9db",
      "f11f57ec439f41bca3e04caea0076035",
      "9c9fd11ebc7a4a4e8d4758aacefcf994",
      "dcb2a8433c59476b9fcfbe57be92bef8",
      "06740f8e44814bf39d199de52072d617",
      "2bc0bcf5b8ef4db89f6723e6111dfab7",
      "9ab21e59fbd141199745dc047b66ccfe",
      "87218eba3fb948d7949ff87d5b9a01e4",
      "c0f7e363ae1a4ac08a87fe9729e99c3d",
      "8ea2b3d8cf954c86b138c93069eaaca5",
      "f52628d8805a4c6c93b073d349f088f5",
      "edb03d46cf2f441ab1b08be095a69572",
      "c7edaba7a40e401cac2437311a68afcd",
      "fd49d84c2ce049e78f2165e69aa40864",
      "948a3815dda549e6b9f547395e9499e7",
      "b929ad33ac154a7c8a1f39a71366558f",
      "4d90847857d342b08091a0f21c2ee394",
      "dd33005a813e4bc1bd6cd64b8b47a29e",
      "763b099544ab4c17af548216f1a9f77c",
      "b4329b496d2447c0bb4a8d5ef0186fee",
      "742b777c25da454ab12aa083d448653a",
      "9188005650fe48f1b896d31bb014ae0c",
      "a5cd220acd1143a2a1d24cd946bf1484",
      "e520002da2ef424bbff2623649f9c874",
      "d388042c525044719a634707e091f43f",
      "f3ae8511d08b4bd0a46f5e3a042170c5",
      "a06e576972064d2aaf2fba731eb87102",
      "a41016ffb35c44a99e38f2e297191d22",
      "d8aed339776b4a9aa68548d0e6d10590",
      "f7341e1b2bcf417f82fb7091842e2db2",
      "a7dd5a6f598149e8ab140375dd038f5a",
      "a00f8600e1c248c8b0549180eaac2613",
      "a4f83a87895e4e9cbdafbe6e2cc808f9",
      "3dd9c49c038341c09f9bf6dff0327062",
      "85aceb326cec4c4bb229e987224d978a",
      "9e2a420ffc2648429574e0c7ec35b521",
      "85ee5f70b2e6474ca75899b29e750070",
      "0a6d46ddc1f5459380e6ff6c67259d8b",
      "3d9f63d03c75443c887b2bc3abcf464b",
      "d5786f54f44f412d93869992e48e6214",
      "e290ccbeb881463389b1594e785d7ccd",
      "ddba667a3a8e4bcd924bf3c873c64a3d",
      "0f34d7b15a444391a074f5c1f874b506",
      "46a8c9cddadf4f46bfc63cc068d738cd",
      "9747e9741291413b8fd88270dd45b96b",
      "68a807fa6eae4bda8b23ad36acdf135a",
      "a5d788c687d74b24a6374e1a824562ca",
      "8fc0c0ca2d2b48f6a330687f42e5d123",
      "6858faaf9bc6458eb75bdbc6eebd39b0",
      "3005a085d6004dde8911782a16bcd9e8",
      "0d5ff633dea74f498ff74da2fb75e555",
      "afd3cf95be7e48b8ac67055db4bfef2c",
      "407cdba0e7f64eefbf4e28e2341d891e",
      "4202eca3a5b04404b4ece197dd60f574",
      "1cf93d884a764d24a73a2f86316940e3",
      "71b7d263a14c47e98d08254182fa798a",
      "cc21a52cdd61437886311904020ffece",
      "f8ab8f014959491db6a361e35fe1c994",
      "814bd65e28bd42a6bfa21e82b6678cd3",
      "7c99e247735949148e6506ded0192b82",
      "d2954890a8de49ff8c696824516ba7d6",
      "870f8a74939b4c73897ce6143551fdbf",
      "3d5ec3c67f6c4fa3816a4f20d65155ad",
      "87d03f5eeca64d52ad734db65487c0df",
      "3da091fda0a144b8ab2bd05822ed7aee",
      "6ddba1982f0946d4b6bbd9e08e368228",
      "dc392cc6ab554979bd2094df21f3a736",
      "5b5bd43f18914974a53289f56b9c4ac3",
      "e8fef71336674d3ba984baffcf0d79f6",
      "2b181d012856460e9fd0dd83e452f393",
      "5e1bcbcce2b04976bc13538cb2ec9ce5",
      "1077bdf029fa458b9cb2ce76dd9836e9",
      "fa3d4977f13c45109bca30ffc7fb22c3",
      "5f9158d5b8a940379c8cb4a11faad657",
      "0f13bcd6fd3e454db382874303b7cde9",
      "f2875819e9ba41f09f8f878a1c719ab6",
      "505de2e27eaa47758269df06b272b36d",
      "a1a5eeafaff0474abcad15d0c1574e47",
      "94c010d06f7148f39cbde295b7a64dd2",
      "e7b2959189ca4c2690007c13244a5609",
      "3d8d5399d127418c90e5faa4d767b9fd",
      "501e7a9cd9da48688608affe78676cd3",
      "ef581d8cfe044e8ebc3bd4622715d81b",
      "cf735abe40334c7dba634bf64a5bfc6a",
      "11960fdb718248aeb9e546fcb3f31845",
      "87f13439cd0e4052bfb8c1b2563c425b",
      "1c41d9e3176445b0842115d30fe51f73",
      "1cb9a1d5637b46e8ac46bf54ce783c87",
      "6d7a956105d74d2facd47ffbca093053",
      "fbf628f99f784454abe395862732b59a",
      "fe7deb5100dc4565bd3d147040140c13",
      "0f20b00022174bfba358005638647364",
      "08a6d555accd467cb6c76299978e49fb",
      "5a0c0d45341547d7bd76be670d4ff666",
      "9ecd2914544f4cc999b0c7c19c163147",
      "571e7ac02cc44274ae88aabe110196cc",
      "c7b3158dfbe54084a442b0d703d9ca08",
      "7a54d46b68fe43fda23f76cfe4724dc1",
      "213eb380e8914bca9f49d0f856d11f31",
      "e65183e0d48e41029861c54dfedb7595",
      "eb91fdde737b417bbf15e2a8abd6a621",
      "731ffedc7a4f408db705f03194df8058",
      "ac169b400a584113b9499cc331b0ff16",
      "4ffaa1c63a40453d8e40ae0a8e905dde",
      "4591d219a6fd4ab690ec58efc4c1d22c",
      "956be3eb96e3402997da24924b19878f",
      "996b5a33b07f409da460df125465e80b",
      "971f887905ad4449875ca33ac981880c",
      "5827aae9298c400c93fdb12951afff32",
      "127e814816d6451da50a0d88a65cee1f",
      "d8acc4105c7140debaaa32c98a095e11",
      "5bad6df9e4fb419bb500403439d1fc56",
      "ac180430a9e74185bc8ce1867c416ac0",
      "02ab06d7d83e4d9b8d571617e8cd7aa3",
      "145df28282fb44ae909bf0aea397264d",
      "2e01bfe5f9da48a2994cb8657a6a7fac",
      "35437416e88f4c4898428d0d3d3e686f",
      "7335ebfb40d34a99a48b740127aefa44",
      "8e46869b06aa4e1ea14868cfbcbc448d",
      "364f394a3f6244da98ca2c6640053c22",
      "f8db32a8371a4dbe9a3fbf0cb485f208",
      "e68afb9f67ce477d9ba955421534bdd0",
      "b1afaaca45554825b1d96991b8b2ca8e",
      "f56132ef375b4c4f9cb3d619304d3812",
      "4554072e1df34508a4b1b644f46687ec",
      "277f848f4640463c8cb168c262c1c87e",
      "c4c80ffbf4ef4d9fbf0939fb7b1436cf",
      "3a01b637e630431fa7f007d12bb4cb1f",
      "48de4cb82d8745ceaac7a09e114abe82",
      "d144022fafb745899a66a2b6736e6420",
      "4e66c66a9580411c9f09b0bb5dddc63a",
      "d44c7869adef4712a8aea6d415967d05",
      "565a97e120d44590bc7c5fec260ddd18",
      "6641d126fb7c4a62add26ddc9c163f49",
      "be1cd99445544306a75ce02de20c1617",
      "296f10f67ee14a1a9d9eeb2af9de6c2b",
      "71601d405755451c8f65bcf0f3770968",
      "7c33d388b1d24b06becc6b1b7c60f6f7",
      "b5ff870cf0954ff3bfb94079576b65a6",
      "5215198edcd246ac861f14e4bfd9066a",
      "6779354257254f72b57d25d13a553063",
      "90d83d1ca9a14c63b566bc14a205e98f",
      "cc7844744c804b40a71f233c9fd9502c",
      "67030e2ecee54bb183239b77e13cb85d",
      "ee3d758f803c4abe901923089ecdf275",
      "9b99784e57cf40ce8516a7efea89d75c",
      "2bebaf5d4df3461d9cd09ca044e5af40",
      "d12ee2cac7b54ad09c3d8ea076a2bb77",
      "00b431a162c74dcb8ffa272be813ebcb",
      "5c67b6793fca45cf9e27290d7c4a03df",
      "03e3cd0e5d624c80b612b92d4a36ca68",
      "e8beb91d3dd949f49c4b99e1609d1c8b",
      "1bd5e0c74b984499b1bc558f6064d0c1",
      "6528605926d542cb947b1f475e9c949b",
      "951d475aee644e94b9d302a50a9ae1b6",
      "02303d52fbca4be384f4a6c5926a0eb1",
      "c1f12dc645d442ae9f58eb3ae6d9a328",
      "46731643e1f84ba49fcf3db47fd5f1d0",
      "58b76b0745324146baee40e075525e80",
      "71183144d12a419887e194442bdcd8fa",
      "08b591e178b94cc2acf8d029beb2d1a6",
      "f4d62529ebfe4508be4f2f19125f714e",
      "4f590102b12a4143b373c61a8b06ed1e",
      "e4b12d375f7843cd83adac23c1e50455",
      "fbec9fa0031045e8a42d483edc6e0b03",
      "11add118fb924f9a9547e19b1da15c1b",
      "f8e5166f88be46398701750959b83171",
      "e0d42c39cc9b4e20bd9fbaf31872faed",
      "9cd0c329b2054d049d1f291bc38eafed",
      "fe7eb49c30f74e888fa4438ba56b2b7f",
      "714729fa673d454aa8d7dc45c407fcfb",
      "03588ed35be644d598cafea4d9affdbc",
      "72b8ff7d5da843aeab7415683400ab57",
      "8d859ca4435944a489bca77e7b9919a7",
      "9d8702b06c8c4cbb878d572264f1bfe0",
      "18ea9830fa114e71a4ff5a9f79b5b007",
      "e55c330d5e4245e6b062f9cacb89e775",
      "18b90e0f01e94b8096ab75ed5421f1f9",
      "6773086ef47d439585ee89d7888bf9ba",
      "9bd508b33f784cb58e9c396626967e18",
      "c821a7daddb84ba4b3dee2d2767a1a13",
      "93d2e574060a4ba5bcd614223983927a",
      "ed6f34f53fbd4455b258643a4cd0c02a",
      "12902e95a5c84d42b99a352cb07d83df",
      "ec4a841056e345ad9aa28a59eb6219d1",
      "a9a75103aa874d6abab7386414b9c427",
      "e770801d28c84a4dbcd2b4ffff58bc5a",
      "60b4e75024b14d61a826041ec9092ba0",
      "0a047e55c9b14812a54b2a7a0bdb46cc",
      "28d8690a196f44aebadf3b99c50de2d4",
      "197ed7fb321a481ca6e420f82fad4794",
      "bc652fdb9b724f408838c11856686d5e",
      "7044a20c0f344c31b6b9bd59c8586171",
      "1c63d777ff8f4b4aa9f4294c43bcbe13",
      "c0a9d2b254cd4cefbb9cef6a39881230",
      "83650723663b4bdda18c5044e58d2e73",
      "a553129af4f44b228976326d327bf009",
      "b66df84f9aa7477bbf293b745f8fb142",
      "93fb8a2a1331407b8113924de73e5903",
      "2497e616d5bf46778b4dd3c36aef9b47",
      "0bb66040295f4c85b788a3036c476a82",
      "4d9420c20ae9483bab6b75c46319c1aa",
      "8cfb0bfe92a34b63aa9bf5a4c5fd63d2",
      "1432eadfdd4a40f6aae7c34c829a40a8",
      "db47de5f43db45cdae52c59bc653d696",
      "e008f5181f4b4c6aa75ec8a9d72b07f8",
      "9f11a35146a4419aaa5f6561a71b2a55",
      "ffdeadb08f804d59aa963fc7deabfd23",
      "26caceb4ebcc41d0b88d8e8aa6ec7dce"
     ]
    },
    "id": "rw09WA182mu4",
    "outputId": "783b1bb2-8960-4b36-f0d5-7acab463e091"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4948acb92c9c492291b662df344d45c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd9b945612c42c5a5060603aea8a15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8288e24299f4078b23c4846d337deb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3260710167417a80305051f3aecd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca5d8e75d604dfebbcba8f15cc0e1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204dc53a701543d6b1403a4fc4cda9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb03d46cf2f441ab1b08be095a69572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cd220acd1143a2a1d24cd946bf1484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd9c49c038341c09f9bf6dff0327062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9747e9741291413b8fd88270dd45b96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b7d263a14c47e98d08254182fa798a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc392cc6ab554979bd2094df21f3a736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a5eeafaff0474abcad15d0c1574e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7a956105d74d2facd47ffbca093053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65183e0d48e41029861c54dfedb7595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8acc4105c7140debaaa32c98a095e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68afb9f67ce477d9ba955421534bdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565a97e120d44590bc7c5fec260ddd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67030e2ecee54bb183239b77e13cb85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6384 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951d475aee644e94b9d302a50a9ae1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11add118fb924f9a9547e19b1da15c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55c330d5e4245e6b062f9cacb89e775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b4e75024b14d61a826041ec9092ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fb8a2a1331407b8113924de73e5903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraghavbhat2004\u001b[0m (\u001b[33mraghavbhat2004-rajarajeshwai-college-of-engineering\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250921_035134-hncazkro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raghavbhat2004-rajarajeshwai-college-of-engineering/huggingface/runs/hncazkro' target=\"_blank\">rose-microwave-3</a></strong> to <a href='https://wandb.ai/raghavbhat2004-rajarajeshwai-college-of-engineering/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raghavbhat2004-rajarajeshwai-college-of-engineering/huggingface' target=\"_blank\">https://wandb.ai/raghavbhat2004-rajarajeshwai-college-of-engineering/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raghavbhat2004-rajarajeshwai-college-of-engineering/huggingface/runs/hncazkro' target=\"_blank\">https://wandb.ai/raghavbhat2004-rajarajeshwai-college-of-engineering/huggingface/runs/hncazkro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 25:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./phi3mini_finetuned/tokenizer_config.json',\n",
       " './phi3mini_finetuned/special_tokens_map.json',\n",
       " './phi3mini_finetuned/chat_template.jinja',\n",
       " './phi3mini_finetuned/tokenizer.model',\n",
       " './phi3mini_finetuned/added_tokens.json',\n",
       " './phi3mini_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set environment variable to reduce GPU memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# Correct model identifier for Phi-3 Mini\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,  # Required for Phi-3 models\n",
    "    use_cache=False # Disable cache to save memory\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Freeze base model parameters for efficient fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Configure LoRA adapters - adjust target_modules for Phi-3 architecture\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Standard attention modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "\n",
    "# Load datasets\n",
    "train_ds = load_dataset(\"json\", data_files=\"train_data.jsonl\")[\"train\"]\n",
    "val_ds = load_dataset(\"json\", data_files=\"validation_data.jsonl\")[\"train\"]\n",
    "\n",
    "# Formatting function\n",
    "def formatting_func(example):\n",
    "    messages = example['conversations']\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[0] if isinstance(msg, (list, tuple)) else msg.get('role')\n",
    "        content = msg[1] if isinstance(msg, (list, tuple)) else msg.get('content')\n",
    "        if role == \"user\":\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    return prompt.strip() # Return the string directly\n",
    "\n",
    "# Training arguments with smaller batch size\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi3mini_finetuned\",\n",
    "    per_device_train_batch_size=1,  # Reduced batch size\n",
    "    gradient_accumulation_steps=32,  # Increased accumulation to compensate for smaller batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./phi3mini_finetuned\")\n",
    "tokenizer.save_pretrained(\"./phi3mini_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be2bfdca"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import re # Import regex for more flexible parsing\n",
    "\n",
    "# Define the model name and output directory\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "output_dir = \"./phi3mini_finetuned\"\n",
    "\n",
    "# Load the base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the fine-tuned model with LoRA adapters\n",
    "print(\"Loading fine-tuned model with LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token\n",
    "\n",
    "# Load the test dataset\n",
    "print(\"Loading test dataset...\")\n",
    "test_ds = load_dataset(\"json\", data_files=\"test_data.jsonl\")[\"train\"]\n",
    "\n",
    "# Function to generate predictions\n",
    "def predict(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        # Disable cache during inference\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1, do_sample=False, use_cache=False)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "# Generate predictions and extract scores\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "print(\"Generating predictions on the test set...\")\n",
    "for example in test_ds:\n",
    "    prompt = example['conversations'][0]['content']\n",
    "    true_label = json.loads(example['conversations'][1]['content'])['relevance_score']\n",
    "\n",
    "    prediction_text = predict(model, tokenizer, prompt)\n",
    "\n",
    "    # Refined logic to extract the relevance score\n",
    "    predicted_score = None\n",
    "    try:\n",
    "        # Attempt to parse as JSON first\n",
    "        score_str = prediction_text.split('Assistant: ')[-1].strip()\n",
    "        predicted_score_dict = json.loads(score_str)\n",
    "        predicted_score = predicted_score_dict.get('relevance_score')\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, try to extract a number using regex\n",
    "        match = re.search(r'\"relevance_score\"\\s*:\\s*([\\d.]+)', prediction_text)\n",
    "        if match:\n",
    "            try:\n",
    "                predicted_score = float(match.group(1))\n",
    "            except ValueError:\n",
    "                print(f\"Could not convert extracted score to float: {match.group(1)}\")\n",
    "                pass # Keep predicted_score as None\n",
    "        else:\n",
    "            print(f\"Could not find 'relevance_score' in prediction: {prediction_text}\")\n",
    "            pass # Keep predicted_score as None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during prediction processing: {e}\")\n",
    "        print(f\"Prediction text: {prediction_text}\")\n",
    "        pass # Keep predicted_score as None\n",
    "\n",
    "\n",
    "    if predicted_score is not None:\n",
    "        predictions.append(predicted_score)\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "if predictions and true_labels:\n",
    "    # Ensure both lists have the same length after skipping\n",
    "    min_len = min(len(predictions), len(true_labels))\n",
    "    predictions = predictions[:min_len]\n",
    "    true_labels = true_labels[:min_len]\n",
    "\n",
    "    mse = mean_squared_error(true_labels, predictions)\n",
    "    r2 = r2_score(true_labels, predictions)\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"R-squared (R2): {r2:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid predictions were generated for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9822f27b",
    "outputId": "960a53a7-92c0-441c-ac99-d8d6c8c955f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory './phi3mini_finetuned' exists.\n",
      "'./phi3mini_finetuned/trainer_state.json' not found. The trainer might not have completed initialization or encountered an early error.\n",
      "Found checkpoints: ['checkpoint-189']\n",
      "Final adapter model files found. Fine-tuning likely completed or saved the final state.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"./phi3mini_finetuned\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"Output directory '{output_dir}' exists.\")\n",
    "    # Check for trainer state file or other indicators of completion\n",
    "    trainer_state_file = os.path.join(output_dir, \"trainer_state.json\")\n",
    "    if os.path.exists(trainer_state_file):\n",
    "        print(f\"'{trainer_state_file}' found. This indicates the trainer was initialized.\")\n",
    "        # You might want to read the trainer_state.json to check for completion status\n",
    "        # However, accessing it directly might be complex.\n",
    "        # A simpler check is to see if the final model files are present or if training logs indicate completion.\n",
    "    else:\n",
    "        print(f\"'{trainer_state_file}' not found. The trainer might not have completed initialization or encountered an early error.\")\n",
    "\n",
    "    # List checkpoints\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d)) and d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        print(f\"Found checkpoints: {checkpoints}\")\n",
    "        # You can further inspect the latest checkpoint for completion indicators if needed.\n",
    "    else:\n",
    "        print(\"No checkpoints found in the output directory.\")\n",
    "\n",
    "    # Check for the final saved model files\n",
    "    if os.path.exists(os.path.join(output_dir, \"adapter_model.safetensors\")) and \\\n",
    "       os.path.exists(os.path.join(output_dir, \"adapter_config.json\")):\n",
    "        print(\"Final adapter model files found. Fine-tuning likely completed or saved the final state.\")\n",
    "    else:\n",
    "        print(\"Final adapter model files not found. Fine-tuning might not have completed.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Output directory '{output_dir}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSeheFCOYtu5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSJVr6dP5Fa1"
   },
   "outputs": [],
   "source": [
    "AIzaSyDjhMXkOZvtIEnjF6jo56cdxmOM11xxYO0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZVQDPhch661S",
    "outputId": "a10cb8df-6f0e-4b37-edba-210576be7c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc4 Upload your resume PDF file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-980289c8-318d-49fa-9829-93053fa9ab0b\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-980289c8-318d-49fa-9829-93053fa9ab0b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving flipkart.pdf to flipkart.pdf\n",
      "\n",
      "\u2702\ufe0f Paste the job description (end with ENTER on a blank line):\n",
      "Role Overview We are seeking a Generative AI Engineer to join our R&D team. In this role, you will design, develop, and deploy state-of-the-art generative models (e.g., GPT, Stable Diffusion, VAE, GANs) to build applications such as text generation, image synthesis, audio cloning, and data augmentation pipelines. You will collaborate with ML researchers, data engineers, and product managers to bring innovative AI capabilities from prototype to production.  Key Responsibilities  Architect and implement generative AI models using frameworks like PyTorch or TensorFlow.  Fine-tune large language models (e.g., GPT-3/4 variants, Falcon, LLaMA, Phi-3) on domain-specific data.  Develop and maintain training pipelines, including data preprocessing, augmentation, and evaluation metrics.  Optimize model inference for low-latency, high-throughput deployment (e.g., quantization, ONNX, Triton).  Collaborate on building RESTful APIs or microservices for AI model serving.  Integrate multimodal capabilities (text-to-image, audio-to-text) into consumer-facing products.  Conduct experimentation, benchmark performance, and iterate on model improvements.  Write clear documentation and present findings to stakeholders.  Qualifications  Bachelor\u2019s or Master\u2019s degree in Computer Science, AI/ML, or a related field.  3+ years of hands-on experience with generative models (GPT, Stable Diffusion, GANs, VAEs).  Proficiency in Python and deep learning frameworks (PyTorch, TensorFlow).  Familiarity with transformers libraries (Hugging Face Transformers, TRL, PEFT).  Experience with scalable training on GPU clusters and cloud platforms (AWS, GCP, Azure).  Knowledge of model optimization techniques (quantization, pruning, mixed precision).  Strong understanding of NLP or computer vision concepts and evaluation metrics.  Excellent problem-solving skills and ability to work in cross-functional teams.  Preferred Skills  Experience with retrieval-augmented generation (RAG) and vector databases (e.g., Pinecone, FAISS).  Background in reinforcement learning or prompt engineering.  Familiarity with MLOps tools (Kubeflow, MLflow, Weights & Biases).  Contributions to open-source AI projects or publications in AI conferences.\n",
      "\n",
      "\n",
      "\ud83d\udd51 Running ATS analysis with Gemini AI\u2026\n",
      "\n",
      "======================================================================\n",
      "\ud83d\udcca ATS ANALYSIS RESULTS\n",
      "======================================================================\n",
      "**1. ATS Score: 35/100**\n",
      "\n",
      "**2. Match Percentage: 25%**\n",
      "\n",
      "**3. Missing Keywords from the Job Description:**\n",
      "\n",
      "* **Generative AI Models (Specifics):** The resume lacks explicit mention of GPT, Stable Diffusion, VAE, or GANs.  While the resume mentions \"GenAI,\" it's not as specific as the job description requires.\n",
      "* **Fine-tuning Large Language Models:**  The resume doesn't showcase experience fine-tuning specific LLMs like GPT-3/4, Falcon, LLaMA, or Phi-3.\n",
      "* **Transformers Libraries:** No mention of Hugging Face Transformers, TRL, or PEFT.\n",
      "* **Model Optimization Techniques:**  The resume mentions optimization in general terms but lacks specific examples of quantization, pruning, or mixed precision.\n",
      "* **NLP/Computer Vision Concepts and Metrics:** The resume is weak in demonstrating a deep understanding of NLP or computer vision concepts and related evaluation metrics (e.g., BLEU, ROUGE, precision, recall, F1-score).\n",
      "* **Retrieval-Augmented Generation (RAG) and Vector Databases:**  The job description lists these as preferred skills, and the resume does not address them.\n",
      "* **Reinforcement Learning or Prompt Engineering:**  Another set of preferred skills missing from the resume.\n",
      "* **MLOps Tools:**  The resume mentions some DevOps tools but lacks specific experience with Kubeflow, MLflow, or Weights & Biases.\n",
      "* **Open-Source Contributions or Publications:** The resume doesn't highlight contributions to open-source projects or publications in AI conferences.\n",
      "\n",
      "\n",
      "**4. Key Strengths in the Resume:**\n",
      "\n",
      "* **Strong foundation in backend development and data engineering:** The resume showcases proficiency in Python, various frameworks (Flask, Spring Boot), databases (SQL, NoSQL), cloud platforms (AWS), and ETL pipelines.\n",
      "* **Experience with REST APIs and Microservices:**  This aligns well with the job description's requirements for building AI model serving infrastructure.\n",
      "* **Experience with AWS:**  The resume shows familiarity with several AWS services relevant to the role.\n",
      "* **Project experience:** Multiple projects demonstrate practical application of skills, although the connection to generative AI is weak.\n",
      "* **Awards and achievements:**  This section adds a positive aspect to the application, highlighting the candidate's accomplishments.\n",
      "\n",
      "\n",
      "**5. Recommendations to Improve the Resume:**\n",
      "\n",
      "* **Quantify accomplishments:** Instead of stating \"improved system scalability by 30%,\" provide concrete numbers \u2013 e.g., \"reduced latency from X to Y milliseconds, resulting in a 30% increase in throughput.\"\n",
      "* **Highlight generative AI experience:**  Explicitly mention any experience with generative models, even if limited.  If projects used any aspects of generative AI, reframe them to showcase this experience prominently.\n",
      "* **Focus on relevant keywords:**  Incorporate the missing keywords identified above.  Don't just list them; demonstrate practical application and quantify achievements using these technologies.\n",
      "* **Expand project descriptions:** Provide more detail on the projects, emphasizing the use of relevant technologies and quantifiable results.  For example, for the \"Shazam Clone,\" explicitly state the types of generative models or techniques used, if any.\n",
      "* **Tailor to the job description:**  Carefully review the job description and adjust the resume's content and structure to directly address each requirement and preferred skill.\n",
      "* **Showcase technical depth:**  Instead of just listing technologies, describe the complexities and challenges faced while using them.\n",
      "* **Add a summary/profile section:** Include a brief summary at the beginning highlighting the most relevant skills and experience for the Generative AI Engineer role.\n",
      "* **Refine the \"Relevant Coursework\" section:**  Mention specific courses related to AI/ML, deep learning, or natural language processing.\n",
      "* **Remove irrelevant skills:**  While a broad skillset is positive, focus on the skills directly relevant to the job description.  Some Java, Spring Boot, Node.js experience could be reduced to make space for relevant information.\n",
      "\n",
      "\n",
      "**6. Skills Analysis (Technical vs. Soft Skills):**\n",
      "\n",
      "**Technical Skills:** The resume exhibits strong technical skills in backend development, data engineering, cloud computing (AWS), and database management. However, it lacks depth in the specific generative AI technologies mentioned in the job description.  The technical skills need to be sharpened to reflect the requirements of the job.\n",
      "\n",
      "**Soft Skills:** The resume implicitly demonstrates some soft skills through project descriptions (teamwork, problem-solving) and achievements. However, these should be explicitly stated, e.g., \"collaborated effectively with a team of X members,\" or \"effectively communicated technical findings to non-technical stakeholders.\"  The resume needs explicit examples of soft skills, rather than just implying them.\n",
      "\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Install dependencies\n",
    "!pip install -q transformers peft PyPDF2\n",
    "\n",
    "# 2. Imports\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from google.colab import files\n",
    "\n",
    "# 3. Load your fine-tuned model and tokenizer\n",
    "model_name_or_path = \"./phi3mini_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 4. PDF text extraction utility\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> str:\n",
    "    from PyPDF2 import PdfReader\n",
    "    reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# 5. Prediction helper\n",
    "def predict(model, tokenizer, prompt: str, max_length: int = 512) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. Upload resume PDF\n",
    "print(\"\ud83d\udcc4 Upload your resume PDF file:\")\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded.\")\n",
    "resume_bytes = next(iter(uploaded.values()))\n",
    "resume_text = extract_text_from_pdf(resume_bytes)\n",
    "\n",
    "# 7. Input job description\n",
    "print(\"\\n\u2702\ufe0f Paste the job description (end with a blank line):\")\n",
    "lines = []\n",
    "while True:\n",
    "    line = input()\n",
    "    if not line.strip():\n",
    "        break\n",
    "    lines.append(line)\n",
    "job_description = \"\\n\".join(lines)\n",
    "\n",
    "# 8. Format prompt for your model (include suggestions)\n",
    "prompt = (\n",
    "    f\"Job Description:\\n{job_description}\\n\\n\"\n",
    "    f\"Resume Text:\\n{resume_text}\\n\\n\"\n",
    "    \"You are an ATS scoring assistant. Analyze the resume vs. job description and output a JSON object with:\\n\"\n",
    "    \"1. relevance_score: a number between 0 and 100\\n\"\n",
    "    \"2. missing_keywords: a list of important keywords from the job description not found in the resume\\n\"\n",
    "    \"3. recommendations: a list of bullet-point suggestions for improving the resume\\n\\n\"\n",
    "    \"Example output:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"relevance_score\\\": 75.4,\\n\"\n",
    "    \"  \\\"missing_keywords\\\": [\\\"AWS\\\", \\\"API development\\\"],\\n\"\n",
    "    \"  \\\"recommendations\\\": [\\n\"\n",
    "    \"    \\\"Add your AWS cloud experience with specific projects\\\",\\n\"\n",
    "    \"    \\\"Include details on API development\\\"\\n\"\n",
    "    \"  ]\\n\"\n",
    "    \"}\"\n",
    ")\n",
    "\n",
    "# 9. Generate prediction\n",
    "predicted_text = predict(model, tokenizer, prompt)\n",
    "\n",
    "# 10. Extract JSON fields\n",
    "predicted_data = {}\n",
    "try:\n",
    "    predicted_data = json.loads(predicted_text)\n",
    "except json.JSONDecodeError:\n",
    "    # Try to extract JSON substring\n",
    "    match = re.search(r'\\{.*\\}', predicted_text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            predicted_data = json.loads(match.group(0))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "relevance_score = predicted_data.get(\"relevance_score\")\n",
    "missing_keywords = predicted_data.get(\"missing_keywords\", [])\n",
    "recommendations = predicted_data.get(\"recommendations\", [])\n",
    "\n",
    "# 11. Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca ATS ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "if relevance_score is not None:\n",
    "    print(f\"Relevance Score: {relevance_score}\")\n",
    "else:\n",
    "    print(\"Could not predict relevance score.\")\n",
    "\n",
    "print(\"\\nMissing Keywords:\")\n",
    "if missing_keywords:\n",
    "    for kw in missing_keywords:\n",
    "        print(f\"- {kw}\")\n",
    "else:\n",
    "    print(\"None detected.\")\n",
    "\n",
    "print(\"\\nRecommendations to Improve Resume:\")\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(f\"- {rec}\")\n",
    "else:\n",
    "    print(\"No recommendations found.\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}